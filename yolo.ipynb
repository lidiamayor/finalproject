{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     61\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m---> 62\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Cargar el modelo YOLO preentrenado\n",
    "model = YOLO('yolov5s.pt')  # Puedes elegir otros modelos como yolov5m.pt, yolov5l.pt, etc.\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "# Capturar video de la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "fatigado = 0\n",
    "patrones_fatigado = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Procesar el frame con YOLO\n",
    "    results = model(frame)\n",
    "\n",
    "    # Obtener las detecciones\n",
    "    detections = results[0].boxes  # Aquí accedemos a las cajas\n",
    "\n",
    "    # Procesar las detecciones\n",
    "    for box in detections:\n",
    "        # La estructura puede variar, pero generalmente incluye las siguientes propiedades\n",
    "        x1, y1, x2, y2 = box.xyxy[0]  # Coordenadas de la caja (x1, y1, x2, y2)\n",
    "        conf = box.conf[0]  # Confianza de la detección\n",
    "        cls = box.cls[0]    # Clase de la detección\n",
    "\n",
    "        if conf > 0.5:# and cls == 0:  # Filtrar por confianza\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)  # Dibuja un rectángulo alrededor del objeto\n",
    "\n",
    "            # Opcionalmente, puedes agregar texto con la clase y la confianza\n",
    "            cv2.putText(frame, f'Clase: {int(cls)} Conf: {conf:.2f}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "            # Extraer la región de interés (ROI) para los ojos\n",
    "            roi_gray = cv2.cvtColor(frame[int(y1):int(y1)+int(y2), int(x1):int(x1)+int(x2)], cv2.COLOR_BGR2GRAY)\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "\n",
    "            for (ex, ey, ew, eh) in eyes:\n",
    "                    # Dibujar los ojos detectados\n",
    "                    cv2.rectangle(frame, (ex, ey), (ex + ew, ey + eh), (255, 0, 0), 2)\n",
    "                    \n",
    "\n",
    "            # Verificar el estado de los ojos\n",
    "            if len(eyes) == 0:\n",
    "                fatigado += 1    \n",
    "            else:\n",
    "                fatigado = 0\n",
    "            if fatigado > 5:\n",
    "                patrones_fatigado += 1\n",
    "                cv2.putText(frame, \"Fatigado\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv2.putText(frame, \"Despierto\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Detección de Fatiga', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movil\n",
    "cls = 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultralytics.engine.results.Boxes object with attributes:\n",
       "\n",
       "cls: tensor([0.])\n",
       "conf: tensor([0.9275])\n",
       "data: tensor([[165.5900,  89.8396, 632.9856, 479.3329,   0.9275,   0.0000]])\n",
       "id: None\n",
       "is_track: False\n",
       "orig_shape: (480, 640)\n",
       "shape: torch.Size([1, 6])\n",
       "xywh: tensor([[399.2878, 284.5862, 467.3956, 389.4933]])\n",
       "xywhn: tensor([[0.6239, 0.5929, 0.7303, 0.8114]])\n",
       "xyxy: tensor([[165.5900,  89.8396, 632.9856, 479.3329]])\n",
       "xyxyn: tensor([[0.2587, 0.1872, 0.9890, 0.9986]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 480x640 1 person, 94.9ms\n",
      "Speed: 3.8ms preprocess, 94.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 163.9ms\n",
      "Speed: 5.2ms preprocess, 163.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 129.7ms\n",
      "Speed: 2.4ms preprocess, 129.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 123.9ms\n",
      "Speed: 2.0ms preprocess, 123.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 120.2ms\n",
      "Speed: 4.2ms preprocess, 120.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lidia Mayor\\AppData\\Local\\Temp\\ipykernel_32932\\2625659710.py:60: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  angle = np.arctan2(gaze_vector[1], gaze_vector[0]) - np.arctan2(road_direction[1], road_direction[0])\n",
      "C:\\Users\\Lidia Mayor\\AppData\\Local\\Temp\\ipykernel_32932\\2625659710.py:61: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  angle = np.degrees(angle)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 480x640 1 person, 124.3ms\n",
      "Speed: 0.5ms preprocess, 124.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 121.8ms\n",
      "Speed: 1.5ms preprocess, 121.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 110.5ms\n",
      "Speed: 1.5ms preprocess, 110.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 105.4ms\n",
      "Speed: 1.0ms preprocess, 105.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.1ms\n",
      "Speed: 1.5ms preprocess, 98.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.3ms\n",
      "Speed: 1.1ms preprocess, 94.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.4ms\n",
      "Speed: 2.2ms preprocess, 96.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 93.1ms\n",
      "Speed: 1.0ms preprocess, 93.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 97.4ms\n",
      "Speed: 2.3ms preprocess, 97.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.4ms\n",
      "Speed: 1.1ms preprocess, 95.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 99.6ms\n",
      "Speed: 0.0ms preprocess, 99.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 98.1ms\n",
      "Speed: 2.4ms preprocess, 98.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 99.3ms\n",
      "Speed: 0.0ms preprocess, 99.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.7ms\n",
      "Speed: 2.0ms preprocess, 96.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 94.0ms\n",
      "Speed: 1.0ms preprocess, 94.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 kite, 97.1ms\n",
      "Speed: 1.0ms preprocess, 97.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.8ms\n",
      "Speed: 0.0ms preprocess, 95.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.9ms\n",
      "Speed: 2.5ms preprocess, 95.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.9ms\n",
      "Speed: 2.0ms preprocess, 94.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.8ms\n",
      "Speed: 1.1ms preprocess, 96.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.4ms\n",
      "Speed: 2.3ms preprocess, 95.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.0ms\n",
      "Speed: 1.2ms preprocess, 97.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.9ms\n",
      "Speed: 1.0ms preprocess, 97.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.7ms\n",
      "Speed: 1.5ms preprocess, 94.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 99.3ms\n",
      "Speed: 0.0ms preprocess, 99.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 100.8ms\n",
      "Speed: 1.0ms preprocess, 100.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.3ms\n",
      "Speed: 0.0ms preprocess, 97.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.4ms\n",
      "Speed: 0.5ms preprocess, 94.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.0ms\n",
      "Speed: 1.0ms preprocess, 98.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 104.1ms\n",
      "Speed: 1.0ms preprocess, 104.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 110.2ms\n",
      "Speed: 1.2ms preprocess, 110.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 104.7ms\n",
      "Speed: 2.3ms preprocess, 104.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.9ms\n",
      "Speed: 1.0ms preprocess, 97.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 95.3ms\n",
      "Speed: 1.0ms preprocess, 95.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 98.1ms\n",
      "Speed: 0.0ms preprocess, 98.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.1ms\n",
      "Speed: 1.5ms preprocess, 98.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.0ms\n",
      "Speed: 1.0ms preprocess, 94.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 99.0ms\n",
      "Speed: 0.0ms preprocess, 99.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.2ms\n",
      "Speed: 1.0ms preprocess, 98.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.4ms\n",
      "Speed: 1.3ms preprocess, 95.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.2ms\n",
      "Speed: 1.5ms preprocess, 96.2ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.4ms\n",
      "Speed: 2.7ms preprocess, 95.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.8ms\n",
      "Speed: 1.3ms preprocess, 97.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.7ms\n",
      "Speed: 1.0ms preprocess, 94.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.4ms\n",
      "Speed: 1.0ms preprocess, 97.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.6ms\n",
      "Speed: 1.5ms preprocess, 95.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.8ms\n",
      "Speed: 0.0ms preprocess, 98.8ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.9ms\n",
      "Speed: 1.0ms preprocess, 95.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 97.1ms\n",
      "Speed: 1.0ms preprocess, 97.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 95.4ms\n",
      "Speed: 1.0ms preprocess, 95.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 93.8ms\n",
      "Speed: 1.5ms preprocess, 93.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.3ms\n",
      "Speed: 1.3ms preprocess, 96.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 99.0ms\n",
      "Speed: 0.0ms preprocess, 99.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 93.7ms\n",
      "Speed: 0.0ms preprocess, 93.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.9ms\n",
      "Speed: 1.0ms preprocess, 95.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 99.5ms\n",
      "Speed: 0.0ms preprocess, 99.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 100.6ms\n",
      "Speed: 1.3ms preprocess, 100.6ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 93.0ms\n",
      "Speed: 1.4ms preprocess, 93.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.2ms\n",
      "Speed: 1.5ms preprocess, 95.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.5ms\n",
      "Speed: 1.0ms preprocess, 95.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 97.0ms\n",
      "Speed: 2.5ms preprocess, 97.0ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.8ms\n",
      "Speed: 1.0ms preprocess, 98.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.4ms\n",
      "Speed: 1.2ms preprocess, 95.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.9ms\n",
      "Speed: 1.3ms preprocess, 95.9ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.4ms\n",
      "Speed: 1.5ms preprocess, 94.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1 bottle, 97.4ms\n",
      "Speed: 0.0ms preprocess, 97.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.1ms\n",
      "Speed: 0.0ms preprocess, 95.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.4ms\n",
      "Speed: 2.3ms preprocess, 95.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.4ms\n",
      "Speed: 1.5ms preprocess, 96.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 94.2ms\n",
      "Speed: 1.0ms preprocess, 94.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 96.4ms\n",
      "Speed: 1.5ms preprocess, 96.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.5ms\n",
      "Speed: 2.3ms preprocess, 98.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.2ms\n",
      "Speed: 1.0ms preprocess, 95.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 98.0ms\n",
      "Speed: 1.0ms preprocess, 98.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 95.7ms\n",
      "Speed: 1.0ms preprocess, 95.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.2ms\n",
      "Speed: 1.0ms preprocess, 136.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Cargar el modelo YOLO preentrenado\n",
    "model = YOLO('yolov5s.pt')\n",
    "\n",
    "# Inicializar la captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Umbral para detectar si el conductor está distraído\n",
    "DISTRACTION_THRESHOLD = 30  # grados\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Procesar el frame con YOLO\n",
    "    results = model(frame)\n",
    "    # Obtener las detecciones\n",
    "    detections = results[0].boxes  # Aquí accedemos a las cajas\n",
    "\n",
    "    # Procesar las detecciones\n",
    "    for box in detections:\n",
    "        # La estructura puede variar, pero generalmente incluye las siguientes propiedades\n",
    "        x1, y1, x2, y2 = box.xyxy[0]  # Coordenadas de la caja (x1, y1, x2, y2)\n",
    "        conf = box.conf[0]  # Confianza de la detección\n",
    "        cls = box.cls[0]    # Clase de la detección\n",
    "        \n",
    "        if cls == 0 and conf > 0.5:  # Clase 0 corresponde a \"persona\"\n",
    "            # Dibuja un rectángulo alrededor del rostro\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "\n",
    "            # Calcular el centro del rostro\n",
    "            face_center_x = (x1 + x2) // 2\n",
    "            face_center_y = (y1 + y2) // 2\n",
    "            \n",
    "            # Aquí puedes agregar lógica para detectar los ojos\n",
    "            # Por ejemplo, usa Haar Cascades o un modelo de detección para los ojos.\n",
    "            eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "            roi_gray = cv2.cvtColor(frame[int(y1):int(y2), int(x1):int(x2)], cv2.COLOR_BGR2GRAY)\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray, 1.3, 5)\n",
    "\n",
    "            if len(eyes) >= 2:  # Necesitamos detectar al menos dos ojos\n",
    "                # Calcular la dirección de la mirada en función de los ojos detectados\n",
    "                for (ex, ey, ew, eh) in eyes:\n",
    "                    # Dibujar los ojos detectados\n",
    "                    cv2.rectangle(frame, (ex, ey), (ex + ew, ey + eh), (255, 0, 0), 2)\n",
    "                    \n",
    "                    # Supongamos que la dirección de la carretera es vertical en el centro del frame\n",
    "                    # Definimos un vector de dirección de la mirada\n",
    "                    gaze_vector = (face_center_x - ex - ew // 2, face_center_y - ey - eh // 2)\n",
    "                    \n",
    "                    # Aquí se puede definir la lógica para calcular el ángulo entre el vector de mirada y la carretera.\n",
    "                    # Esto requiere conocer la orientación de la carretera.\n",
    "                    # En este ejemplo, vamos a simular esto.\n",
    "                    \n",
    "                    road_direction = (0, 1)  # Por ejemplo, hacia abajo\n",
    "                    angle = np.arctan2(gaze_vector[1], gaze_vector[0]) - np.arctan2(road_direction[1], road_direction[0])\n",
    "                    angle = np.degrees(angle)\n",
    "\n",
    "                    # Comprobar si el ángulo excede el umbral de distracción\n",
    "                    if abs(angle) > DISTRACTION_THRESHOLD:\n",
    "                        cv2.putText(frame, \"Distraccion\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    else:\n",
    "                        cv2.putText(frame, \"Atento\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Detección de Distracción', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
